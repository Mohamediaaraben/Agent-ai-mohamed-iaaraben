{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebSurfer Agent with Headless GUI-based Browsing\n",
    "\n",
    "This notebook is derived from the standard [WebSurferAgent Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_surfer.ipynb) for the purposes of demonstrating coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "AutoGen requires `Python>=3.8`. To run this notebook example, please install:\n",
    "```bash\n",
    "pip install pyautogen selenium markdownify pillow pdfminer.six beautifulsoup4 arxiv\n",
    "```\n",
    "or\n",
    "```bash\n",
    "pip install \"pyautogen[websurfer]\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure that we have the WebDrivers present for Selenium\n",
    "\n",
    "*EDIT*:\n",
    "[Selenium Manager](https://www.selenium.dev/documentation/selenium_manager/) states:\n",
    "\"Selenium Manager is a command-line tool implemented in Rust that provides automated driver and browser management for Selenium. Selenium bindings use this tool by default, so you do not need to download it or add anything to your code or do anything else to use it.\"\n",
    "\n",
    "Therefore the folling instructions should not be needed:\n",
    "Following the instructions in [Selenium Documentation](https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location/#download-the-driver), \n",
    "we first download the web driver for our browser of choice, or all 3: [Edge](https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver/?form=MA13LH#downloads), [Firefox](https://github.com/mozilla/geckodriver/releases), [Chrome](https://chromedriver.chromium.org/downloads).~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neither powershell nor pwsh is installed.\n"
     ]
    }
   ],
   "source": [
    "# %%capture --no-stderr\n",
    "import os\n",
    "import logging\n",
    "import autogen\n",
    "from time import sleep\n",
    "\n",
    "from autogen.agentchat.contrib.web_surfer import WebSurferAgent\n",
    "from autogen.agentchat.conversable_agent import ConversableAgent\n",
    "from autogen.agentchat.user_proxy_agent import UserProxyAgent\n",
    "from autogen.oai import config_list_from_json\n",
    "from autogen.browser_utils import display_binary_image\n",
    "\n",
    "# Get the logger instance for the current module (__name__).\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your API Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`config_list_from_json`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file.\n",
    "\n",
    "It first looks for environment variable \"OAI_CONFIG_LIST\" which needs to be a valid json string. If that variable is not found, it then looks for a json file named \"OAI_CONFIG_LIST\". It filters the configs by models (you can filter by other keys as well).\n",
    "\n",
    "The WebSurferAgent uses a combination of models. GPT-4 and GPT-3.5-turbo-16 are recommended.\n",
    "\n",
    "Your json config should look something like the following:\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"model\": \"gpt-4\",\n",
    "        \"api_key\": \"<your OpenAI API key here>\"\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"gpt-3.5-turbo-16k\",\n",
    "        \"api_key\": \"<your OpenAI API key here>\"\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "If you open this notebook in colab, you can upload your files by clicking the file icon on the left panel and then choose \"upload file\" icon.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = {\n",
    "    \"timeout\": 600,\n",
    "    \"cache_seed\": 44,  # change the seed for different trials\n",
    "    \"config_list\": config_list_from_json(\n",
    "        \"OAI_CONFIG_LIST\",\n",
    "        filter_dict={\"model\": [\"gpt-3.5-turbo\"]},\n",
    "    ),\n",
    "    \"temperature\": 0,\n",
    "}\n",
    "\n",
    "summarizer_llm_config = {\n",
    "    \"timeout\": 600,\n",
    "    \"cache_seed\": 44,  # change the seed for different trials\n",
    "    \"config_list\": config_list_from_json(\n",
    "        \"OAI_CONFIG_LIST\",\n",
    "        filter_dict={\"model\": [\"gpt-3.5-turbo\"]},\n",
    "    ),\n",
    "    \"temperature\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Bing\n",
    "\n",
    "For WebSurferAgent to be reasonably useful, it needs to be able to search the web -- and that means it needs a Bing API key. \n",
    "You can read more about how to get an API on the [Bing Web Search API](https://www.microsoft.com/en-us/bing/apis/bing-web-search-api) page.\n",
    "\n",
    "Once you have your key, either set it as the `BING_API_KEY` system environment variable, or simply input your key below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bing_api_key = os.environ[\"BING_API_KEY\"] if \"BING_API_KEY\" in os.environ else \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Agents\n",
    "\n",
    "We now create out WebSurferAgent, and a UserProxyAgent to surf the web, but using a graphical based browser required for many use-cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_surfer = WebSurferAgent(\n",
    "    \"web_surfer\",\n",
    "    llm_config=llm_config,\n",
    "    summarizer_llm_config=summarizer_llm_config,\n",
    "    browser_config={\n",
    "        \"type\": \"selenium\",  # *NEW* Here we specify that we intend to use our headless GUI browser. The default setting is \"text\".\n",
    "        \"browser\": \"edge\",  # *NEW* We'll use the edge browser for these tests.  Choices include 'edge', 'firefox', and 'chrome'\n",
    "        \"resolution\": (1400, 900),  # *NEW* we specify the browser window size.  The default is (1920,5200)\n",
    "        \"render_text\": False,  # *NEW* We still have the option to convert the output to text and render it in the browser\n",
    "        \"bing_api_key\": bing_api_key,\n",
    "    },\n",
    ")\n",
    "\n",
    "user_proxy = UserProxyAgent(\n",
    "    \"user_proxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config=False,\n",
    "    default_auto_reply=\"\",\n",
    "    is_termination_msg=lambda x: True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Search, summarize\n",
    "- Search for information aobut Microsoft AutoGen\n",
    "- Summarize the results\n",
    "- Visit the Getting Started Docs page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to web_surfer):\n",
      "\n",
      "\n",
      "Search the web for information about Microsoft AutoGen\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION informational_web_search...\u001b[0m\n",
      "\u001b[33mweb_surfer\u001b[0m (to user_proxy):\n",
      "\n",
      "Address: bing: Microsoft AutoGen\n",
      "Title: Microsoft AutoGen - Search\n",
      "Viewport position: Showing page 1 of 1.\n",
      "=======================\n",
      "A Bing search for 'Microsoft AutoGen' found 8 results:\n",
      "\n",
      "## Web Results\n",
      "1. [AutoGen: Enabling next-generation large language model applications](https://www.microsoft.com/en-us/research/blog/autogen-enabling-next-generation-large-language-model-applications/)\n",
      "AutoGen is a Python package that simplifies the orchestration, optimization, and automation of large language model applications. It enables customizable and conversable agents that integrate with humans, tools, and other agents to solve tasks using GPT-4 and other advanced LLMs. Learn how to use AutoGen for code-based question answering, supply-chain optimization, conversational chess, and more.\n",
      "\n",
      "2. [GitHub - microsoft/autogen: Enable Next-Gen Large Language Model ...](https://github.com/microsoft/autogen)\n",
      "AutoGen is a framework that enables the development of large language model applications using multiple agents that can converse with each other to solve tasks. It supports diverse conversation patterns, enhanced LLM inference, and customizable and conversable agents.\n",
      "\n",
      "3. [Getting Started | AutoGen - microsoft.github.io](https://microsoft.github.io/autogen/docs/Getting-Started/)\n",
      "AutoGen is a framework that enables development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools. Main Features\n",
      "\n",
      "4. [AutoGen | AutoGen - microsoft.github.io](https://microsoft.github.io/autogen/)\n",
      "AutoGen is a tool that enables next-gen large language model applications by providing a high-level abstraction for building diverse and enhanced LLM workflows. It offers a collection of working systems for various domains and complexities, as well as enhanced LLM inference and optimization APIs.\n",
      "\n",
      "5. [AutoGen Studio: Interactively Explore Multi-Agent Workflows](https://microsoft.github.io/autogen/blog/2023/12/01/AutoGenStudio/)\n",
      "AutoGen has emerged as a leading framework for orchestrating the power of agents. In the spirit of expanding this frontier and democratizing this capability, we are thrilled to introduce a new user-friendly interface: AutoGen Studio.\n",
      "\n",
      "6. [[2308.08155] AutoGen: Enabling Next-Gen LLM Applications via Multi ...](https://arxiv.org/abs/2308.08155)\n",
      "AutoGen is an open-source framework that allows developers to create and customize agents that can converse with each other to perform tasks using various types of language models (LLMs). The framework supports natural language and code-based conversation patterns, and is effective for diverse applications such as mathematics, coding, question answering, and more.\n",
      "\n",
      "7. [Mastering AutoGen: A Comprehensive Guide to Next-Generation ... - Medium](https://medium.com/@krtarunsingh/mastering-autogen-a-comprehensive-guide-to-next-generation-language-model-applications-b375d9b4dc6d)\n",
      "AutoGen is a framework by Microsoft that allows you to create applications that leverage large language models (LLMs) with multi-agent conversations, diverse patterns, and enhanced inference. Learn how to set up AutoGen, use its architecture, and apply its features in this comprehensive guide by Tarun Singh.\n",
      "\n",
      "8. [arXiv:2308.08155v2 [cs.AI] 3 Oct 2023](https://arxiv.org/pdf/2308.08155.pdf)\n",
      "AutoGen is an open-source framework that allows developers to create and customize agents that can converse with each other to solve tasks using multiple languages, tools, and human inputs. The framework supports flexible conversation patterns and natural or code-based programming for diverse applications of complexities and LLM capacities.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_history=[{'content': '\\nSearch the web for information about Microsoft AutoGen\\n', 'role': 'assistant'}, {'content': \"Address: bing: Microsoft AutoGen\\nTitle: Microsoft AutoGen - Search\\nViewport position: Showing page 1 of 1.\\n=======================\\nA Bing search for 'Microsoft AutoGen' found 8 results:\\n\\n## Web Results\\n1. [AutoGen: Enabling next-generation large language model applications](https://www.microsoft.com/en-us/research/blog/autogen-enabling-next-generation-large-language-model-applications/)\\nAutoGen is a Python package that simplifies the orchestration, optimization, and automation of large language model applications. It enables customizable and conversable agents that integrate with humans, tools, and other agents to solve tasks using GPT-4 and other advanced LLMs. Learn how to use AutoGen for code-based question answering, supply-chain optimization, conversational chess, and more.\\n\\n2. [GitHub - microsoft/autogen: Enable Next-Gen Large Language Model ...](https://github.com/microsoft/autogen)\\nAutoGen is a framework that enables the development of large language model applications using multiple agents that can converse with each other to solve tasks. It supports diverse conversation patterns, enhanced LLM inference, and customizable and conversable agents.\\n\\n3. [Getting Started | AutoGen - microsoft.github.io](https://microsoft.github.io/autogen/docs/Getting-Started/)\\nAutoGen is a framework that enables development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools. Main Features\\n\\n4. [AutoGen | AutoGen - microsoft.github.io](https://microsoft.github.io/autogen/)\\nAutoGen is a tool that enables next-gen large language model applications by providing a high-level abstraction for building diverse and enhanced LLM workflows. It offers a collection of working systems for various domains and complexities, as well as enhanced LLM inference and optimization APIs.\\n\\n5. [AutoGen Studio: Interactively Explore Multi-Agent Workflows](https://microsoft.github.io/autogen/blog/2023/12/01/AutoGenStudio/)\\nAutoGen has emerged as a leading framework for orchestrating the power of agents. In the spirit of expanding this frontier and democratizing this capability, we are thrilled to introduce a new user-friendly interface: AutoGen Studio.\\n\\n6. [[2308.08155] AutoGen: Enabling Next-Gen LLM Applications via Multi ...](https://arxiv.org/abs/2308.08155)\\nAutoGen is an open-source framework that allows developers to create and customize agents that can converse with each other to perform tasks using various types of language models (LLMs). The framework supports natural language and code-based conversation patterns, and is effective for diverse applications such as mathematics, coding, question answering, and more.\\n\\n7. [Mastering AutoGen: A Comprehensive Guide to Next-Generation ... - Medium](https://medium.com/@krtarunsingh/mastering-autogen-a-comprehensive-guide-to-next-generation-language-model-applications-b375d9b4dc6d)\\nAutoGen is a framework by Microsoft that allows you to create applications that leverage large language models (LLMs) with multi-agent conversations, diverse patterns, and enhanced inference. Learn how to set up AutoGen, use its architecture, and apply its features in this comprehensive guide by Tarun Singh.\\n\\n8. [arXiv:2308.08155v2 [cs.AI] 3 Oct 2023](https://arxiv.org/pdf/2308.08155.pdf)\\nAutoGen is an open-source framework that allows developers to create and customize agents that can converse with each other to solve tasks using multiple languages, tools, and human inputs. The framework supports flexible conversation patterns and natural or code-based programming for diverse applications of complexities and LLM capacities.\", 'role': 'user'}], summary=\"Address: bing: Microsoft AutoGen\\nTitle: Microsoft AutoGen - Search\\nViewport position: Showing page 1 of 1.\\n=======================\\nA Bing search for 'Microsoft AutoGen' found 8 results:\\n\\n## Web Results\\n1. [AutoGen: Enabling next-generation large language model applications](https://www.microsoft.com/en-us/research/blog/autogen-enabling-next-generation-large-language-model-applications/)\\nAutoGen is a Python package that simplifies the orchestration, optimization, and automation of large language model applications. It enables customizable and conversable agents that integrate with humans, tools, and other agents to solve tasks using GPT-4 and other advanced LLMs. Learn how to use AutoGen for code-based question answering, supply-chain optimization, conversational chess, and more.\\n\\n2. [GitHub - microsoft/autogen: Enable Next-Gen Large Language Model ...](https://github.com/microsoft/autogen)\\nAutoGen is a framework that enables the development of large language model applications using multiple agents that can converse with each other to solve tasks. It supports diverse conversation patterns, enhanced LLM inference, and customizable and conversable agents.\\n\\n3. [Getting Started | AutoGen - microsoft.github.io](https://microsoft.github.io/autogen/docs/Getting-Started/)\\nAutoGen is a framework that enables development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools. Main Features\\n\\n4. [AutoGen | AutoGen - microsoft.github.io](https://microsoft.github.io/autogen/)\\nAutoGen is a tool that enables next-gen large language model applications by providing a high-level abstraction for building diverse and enhanced LLM workflows. It offers a collection of working systems for various domains and complexities, as well as enhanced LLM inference and optimization APIs.\\n\\n5. [AutoGen Studio: Interactively Explore Multi-Agent Workflows](https://microsoft.github.io/autogen/blog/2023/12/01/AutoGenStudio/)\\nAutoGen has emerged as a leading framework for orchestrating the power of agents. In the spirit of expanding this frontier and democratizing this capability, we are thrilled to introduce a new user-friendly interface: AutoGen Studio.\\n\\n6. [[2308.08155] AutoGen: Enabling Next-Gen LLM Applications via Multi ...](https://arxiv.org/abs/2308.08155)\\nAutoGen is an open-source framework that allows developers to create and customize agents that can converse with each other to perform tasks using various types of language models (LLMs). The framework supports natural language and code-based conversation patterns, and is effective for diverse applications such as mathematics, coding, question answering, and more.\\n\\n7. [Mastering AutoGen: A Comprehensive Guide to Next-Generation ... - Medium](https://medium.com/@krtarunsingh/mastering-autogen-a-comprehensive-guide-to-next-generation-language-model-applications-b375d9b4dc6d)\\nAutoGen is a framework by Microsoft that allows you to create applications that leverage large language models (LLMs) with multi-agent conversations, diverse patterns, and enhanced inference. Learn how to set up AutoGen, use its architecture, and apply its features in this comprehensive guide by Tarun Singh.\\n\\n8. [arXiv:2308.08155v2 [cs.AI] 3 Oct 2023](https://arxiv.org/pdf/2308.08155.pdf)\\nAutoGen is an open-source framework that allows developers to create and customize agents that can converse with each other to solve tasks using multiple languages, tools, and human inputs. The framework supports flexible conversation patterns and natural or code-based programming for diverse applications of complexities and LLM capacities.\", cost=({'total_cost': 0}, {'total_cost': 0}), human_input=[])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bing search is a special case and we return the text in the same way as the SimpleTextBrowser\n",
    "\n",
    "task1 = \"\"\"\n",
    "Search the web for information about Microsoft AutoGen\n",
    "\"\"\"\n",
    "\n",
    "user_proxy.initiate_chat(web_surfer, message=task1)\n",
    "\n",
    "# Note that these results are also accessable in JSON format with `web_surfer.browser.bing_results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to web_surfer):\n",
      "\n",
      "Summarize these results\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION summarize_page...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The token limit (4096) of the WebSurferAgent.summarizer_llm_config, is below the recommended 16k.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mweb_surfer\u001b[0m (to user_proxy):\n",
      "\n",
      "AutoGen is a framework developed by Microsoft Research to simplify the orchestration, optimization, and automation of large language model (LLM) workflows. The framework offers customizable and conversable agents that utilize advanced LLM capabilities, such as GPT-4, while also integrating with humans and tools to address limitations and enhance performance. As developers create more complex LLM-based applications, the workflows become intricate, requiring significant effort and expertise to design and implement. Automating these workflows using AutoGen can streamline the process and improve efficiency, enabling the creation of next-generation applications that leverage the full potential of LLMs. The framework supports conversations between multiple agents through automated chat, providing a solution to the challenge of orchestrating optimal workflows in a vast and complex design space.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_history=[{'content': '\\nSearch the web for information about Microsoft AutoGen\\n', 'role': 'assistant'}, {'content': \"Address: bing: Microsoft AutoGen\\nTitle: Microsoft AutoGen - Search\\nViewport position: Showing page 1 of 1.\\n=======================\\nA Bing search for 'Microsoft AutoGen' found 8 results:\\n\\n## Web Results\\n1. [AutoGen: Enabling next-generation large language model applications](https://www.microsoft.com/en-us/research/blog/autogen-enabling-next-generation-large-language-model-applications/)\\nAutoGen is a Python package that simplifies the orchestration, optimization, and automation of large language model applications. It enables customizable and conversable agents that integrate with humans, tools, and other agents to solve tasks using GPT-4 and other advanced LLMs. Learn how to use AutoGen for code-based question answering, supply-chain optimization, conversational chess, and more.\\n\\n2. [GitHub - microsoft/autogen: Enable Next-Gen Large Language Model ...](https://github.com/microsoft/autogen)\\nAutoGen is a framework that enables the development of large language model applications using multiple agents that can converse with each other to solve tasks. It supports diverse conversation patterns, enhanced LLM inference, and customizable and conversable agents.\\n\\n3. [Getting Started | AutoGen - microsoft.github.io](https://microsoft.github.io/autogen/docs/Getting-Started/)\\nAutoGen is a framework that enables development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools. Main Features\\n\\n4. [AutoGen | AutoGen - microsoft.github.io](https://microsoft.github.io/autogen/)\\nAutoGen is a tool that enables next-gen large language model applications by providing a high-level abstraction for building diverse and enhanced LLM workflows. It offers a collection of working systems for various domains and complexities, as well as enhanced LLM inference and optimization APIs.\\n\\n5. [AutoGen Studio: Interactively Explore Multi-Agent Workflows](https://microsoft.github.io/autogen/blog/2023/12/01/AutoGenStudio/)\\nAutoGen has emerged as a leading framework for orchestrating the power of agents. In the spirit of expanding this frontier and democratizing this capability, we are thrilled to introduce a new user-friendly interface: AutoGen Studio.\\n\\n6. [[2308.08155] AutoGen: Enabling Next-Gen LLM Applications via Multi ...](https://arxiv.org/abs/2308.08155)\\nAutoGen is an open-source framework that allows developers to create and customize agents that can converse with each other to perform tasks using various types of language models (LLMs). The framework supports natural language and code-based conversation patterns, and is effective for diverse applications such as mathematics, coding, question answering, and more.\\n\\n7. [Mastering AutoGen: A Comprehensive Guide to Next-Generation ... - Medium](https://medium.com/@krtarunsingh/mastering-autogen-a-comprehensive-guide-to-next-generation-language-model-applications-b375d9b4dc6d)\\nAutoGen is a framework by Microsoft that allows you to create applications that leverage large language models (LLMs) with multi-agent conversations, diverse patterns, and enhanced inference. Learn how to set up AutoGen, use its architecture, and apply its features in this comprehensive guide by Tarun Singh.\\n\\n8. [arXiv:2308.08155v2 [cs.AI] 3 Oct 2023](https://arxiv.org/pdf/2308.08155.pdf)\\nAutoGen is an open-source framework that allows developers to create and customize agents that can converse with each other to solve tasks using multiple languages, tools, and human inputs. The framework supports flexible conversation patterns and natural or code-based programming for diverse applications of complexities and LLM capacities.\", 'role': 'user'}, {'content': 'Summarize these results', 'role': 'assistant'}, {'content': 'AutoGen is a framework developed by Microsoft Research to simplify the orchestration, optimization, and automation of large language model (LLM) workflows. The framework offers customizable and conversable agents that utilize advanced LLM capabilities, such as GPT-4, while also integrating with humans and tools to address limitations and enhance performance. As developers create more complex LLM-based applications, the workflows become intricate, requiring significant effort and expertise to design and implement. Automating these workflows using AutoGen can streamline the process and improve efficiency, enabling the creation of next-generation applications that leverage the full potential of LLMs. The framework supports conversations between multiple agents through automated chat, providing a solution to the challenge of orchestrating optimal workflows in a vast and complex design space.', 'role': 'user'}], summary='AutoGen is a framework developed by Microsoft Research to simplify the orchestration, optimization, and automation of large language model (LLM) workflows. The framework offers customizable and conversable agents that utilize advanced LLM capabilities, such as GPT-4, while also integrating with humans and tools to address limitations and enhance performance. As developers create more complex LLM-based applications, the workflows become intricate, requiring significant effort and expertise to design and implement. Automating these workflows using AutoGen can streamline the process and improve efficiency, enabling the creation of next-generation applications that leverage the full potential of LLMs. The framework supports conversations between multiple agents through automated chat, providing a solution to the challenge of orchestrating optimal workflows in a vast and complex design space.', cost=({'total_cost': 0}, {'total_cost': 0}), human_input=[])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task2 = \"Summarize these results\"\n",
    "user_proxy.initiate_chat(web_surfer, message=task2, clear_history=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to web_surfer):\n",
      "\n",
      "Click the 'Getting Started' result\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION visit_page...\u001b[0m\n",
      "\u001b[33mweb_surfer\u001b[0m (to user_proxy):\n",
      "\n",
      "Address: https://microsoft.github.io/autogen/docs/Getting-Started/\n",
      "Title: Getting Started | AutoGen\n",
      "Viewport position: Showing page 1 of 1.\n",
      "=======================\n",
      "\n",
      "\n",
      "\n",
      "Getting Started | AutoGen\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Skip to main content](#__docusaurus_skipToContent_fallback)[![AutoGen](/autogen/img/ag.svg)**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)[Resources](#)* [Ecosystem](/autogen/docs/Ecosystem)\n",
      "* [Gallery](/autogen/docs/Gallery)\n",
      "[Other Languages](#)* [Dotnet](https://microsoft.github.io/autogen-for-net/)\n",
      "[GitHub](https://github.com/microsoft/autogen)`⌘``K`* [Getting Started](/autogen/docs/Getting-Started)\n",
      "* [Installation](/autogen/docs/installation/)\n",
      "* [LLM Configuration](/autogen/docs/llm_configuration)\n",
      "* [Use Cases](#)\n",
      "* [Contributing](/autogen/docs/Contribute)\n",
      "* [Research](/autogen/docs/Research)\n",
      "* [Migration Guide](/autogen/docs/Migration-Guide)\n",
      "* \n",
      "* Getting Started\n",
      "On this pageGetting Started\n",
      "===============\n",
      "\n",
      "\n",
      "AutoGen is a framework that enables development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools.\n",
      "\n",
      "\n",
      "![AutoGen Overview](/autogen/assets/images/autogen_agentchat-250ca64b77b87e70d34766a080bf6ba8.png)\n",
      "\n",
      "\n",
      "### Main Features[​](#main-features \"Direct link to Main Features\")\n",
      "\n",
      "\n",
      "* AutoGen enables building next-gen LLM applications based on [multi-agent conversations](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat) with minimal effort. It simplifies the orchestration, automation, and optimization of a complex LLM workflow. It maximizes the performance of LLM models and overcomes their weaknesses.\n",
      "* It supports [diverse conversation patterns](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat#supporting-diverse-conversation-patterns) for complex workflows. With customizable and conversable agents, developers can use AutoGen to build a wide range of conversation patterns concerning conversation autonomy,\n",
      "the number of agents, and agent conversation topology.\n",
      "* It provides a collection of working systems with different complexities. These systems span a [wide range of applications](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat#diverse-applications-implemented-with-autogen) from various domains and complexities. This demonstrates how AutoGen can easily support diverse conversation patterns.\n",
      "* AutoGen provides [enhanced LLM inference](https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference#api-unification). It offers utilities like API unification and caching, and advanced usage patterns, such as error handling, multi-config inference, context programming, etc.\n",
      "\n",
      "\n",
      "AutoGen is powered by collaborative [research studies](/autogen/docs/Research) from Microsoft, Penn State University, and University of Washington.\n",
      "\n",
      "\n",
      "### Quickstart[​](#quickstart \"Direct link to Quickstart\")\n",
      "\n",
      "\n",
      "Install from pip: `pip install pyautogen`. Find more options in [Installation](/autogen/docs/installation/).\n",
      "For [code execution](/autogen/docs/FAQ#code-execution), we strongly recommend installing the python docker package, and using docker.\n",
      "\n",
      "\n",
      "#### Multi-Agent Conversation Framework[​](#multi-agent-conversation-framework \"Direct link to Multi-Agent Conversation Framework\")\n",
      "\n",
      "\n",
      "Autogen enables the next-gen LLM applications with a generic multi-agent conversation framework. It offers customizable and conversable agents which integrate LLMs, tools, and humans.\n",
      "By automating chat among multiple capable agents, one can easily make them collectively perform tasks autonomously or with human feedback, including tasks that require using tools via code. For [example](https://github.com/microsoft/autogen/blob/main/test/twoagent.py),\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "from autogen import AssistantAgent, UserProxyAgent, config\\_list\\_from\\_json  \n",
      "  \n",
      "# Load LLM inference endpoints from an env variable or a file  \n",
      "# See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints  \n",
      "# and OAI\\_CONFIG\\_LIST\\_sample.json  \n",
      "config\\_list = config\\_list\\_from\\_json(env\\_or\\_file=\"OAI\\_CONFIG\\_LIST\")  \n",
      "assistant = AssistantAgent(\"assistant\", llm\\_config={\"config\\_list\": config\\_list})  \n",
      "user\\_proxy = UserProxyAgent(\"user\\_proxy\", code\\_execution\\_config={\"work\\_dir\": \"coding\", \"use\\_docker\": False}) # IMPORTANT: set to True to run code in docker, recommended  \n",
      "user\\_proxy.initiate\\_chat(assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\")  \n",
      "# This initiates an automated chat between the two agents to solve the task  \n",
      "\n",
      "```\n",
      "\n",
      "The figure below shows an example conversation flow with AutoGen.\n",
      "![Agent Chat Example](/autogen/assets/images/chat_example-da70a7420ebc817ef9826fa4b1e80951.png)\n",
      "\n",
      "\n",
      "* [Code examples](/autogen/docs/Examples).\n",
      "* [Documentation](/autogen/docs/Use-Cases/agent_chat).\n",
      "\n",
      "\n",
      "#### Enhanced LLM Inferences[​](#enhanced-llm-inferences \"Direct link to Enhanced LLM Inferences\")\n",
      "\n",
      "\n",
      "Autogen also helps maximize the utility out of the expensive LLMs such as ChatGPT and GPT-4. It offers enhanced LLM inference with powerful functionalities like tuning, caching, error handling, templating. For example, you can optimize generations by LLM with your own tuning data, success metrics and budgets.\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "# perform tuning for openai<1  \n",
      "config, analysis = autogen.Completion.tune(  \n",
      " data=tune\\_data,  \n",
      " metric=\"success\",  \n",
      " mode=\"max\",  \n",
      " eval\\_func=eval\\_func,  \n",
      " inference\\_budget=0.05,  \n",
      " optimization\\_budget=3,  \n",
      " num\\_samples=-1,  \n",
      ")  \n",
      "# perform inference for a test instance  \n",
      "response = autogen.Completion.create(context=test\\_instance, \\*\\*config)  \n",
      "\n",
      "```\n",
      "\n",
      "* [Code examples](/autogen/docs/Examples).\n",
      "* [Documentation](/autogen/docs/Use-Cases/enhanced_inference).\n",
      "\n",
      "\n",
      "### Where to Go Next ?[​](#where-to-go-next- \"Direct link to Where to Go Next ?\")\n",
      "\n",
      "\n",
      "* Understand the use cases for [multi-agent conversation](/autogen/docs/Use-Cases/agent_chat) and [enhanced LLM inference](/autogen/docs/Use-Cases/enhanced_inference).\n",
      "* Find [code examples](/autogen/docs/Examples).\n",
      "* Read [SDK](/autogen/docs/reference/agentchat/conversable_agent/).\n",
      "* Learn about [research](/autogen/docs/Research) around AutoGen.\n",
      "* [Roadmap](https://github.com/orgs/microsoft/projects/989/views/3)\n",
      "* Chat on [Discord](https://discord.gg/pAbnFJrkgZ).\n",
      "* Follow on [Twitter](https://twitter.com/pyautogen).\n",
      "\n",
      "\n",
      "If you like our project, please give it a [star](https://github.com/microsoft/autogen/stargazers) on GitHub. If you are interested in contributing, please read [Contributor's Guide](/autogen/docs/Contribute).\n",
      "\n",
      "\n",
      "[Edit this page](https://github.com/microsoft/autogen/edit/main/website/docs/Getting-Started.md)[NextInstallation](/autogen/docs/installation/)* [Main Features](#main-features)\n",
      "* [Quickstart](#quickstart)\n",
      "* [Where to Go Next ?](#where-to-go-next-)\n",
      "Community* [Discord](https://discord.gg/pAbnFJrkgZ)\n",
      "* [Twitter](https://twitter.com/pyautogen)\n",
      "Copyright © 2024 AutoGen Authors | [Privacy and Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_history=[{'content': '\\nSearch the web for information about Microsoft AutoGen\\n', 'role': 'assistant'}, {'content': \"Address: bing: Microsoft AutoGen\\nTitle: Microsoft AutoGen - Search\\nViewport position: Showing page 1 of 1.\\n=======================\\nA Bing search for 'Microsoft AutoGen' found 8 results:\\n\\n## Web Results\\n1. [AutoGen: Enabling next-generation large language model applications](https://www.microsoft.com/en-us/research/blog/autogen-enabling-next-generation-large-language-model-applications/)\\nAutoGen is a Python package that simplifies the orchestration, optimization, and automation of large language model applications. It enables customizable and conversable agents that integrate with humans, tools, and other agents to solve tasks using GPT-4 and other advanced LLMs. Learn how to use AutoGen for code-based question answering, supply-chain optimization, conversational chess, and more.\\n\\n2. [GitHub - microsoft/autogen: Enable Next-Gen Large Language Model ...](https://github.com/microsoft/autogen)\\nAutoGen is a framework that enables the development of large language model applications using multiple agents that can converse with each other to solve tasks. It supports diverse conversation patterns, enhanced LLM inference, and customizable and conversable agents.\\n\\n3. [Getting Started | AutoGen - microsoft.github.io](https://microsoft.github.io/autogen/docs/Getting-Started/)\\nAutoGen is a framework that enables development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools. Main Features\\n\\n4. [AutoGen | AutoGen - microsoft.github.io](https://microsoft.github.io/autogen/)\\nAutoGen is a tool that enables next-gen large language model applications by providing a high-level abstraction for building diverse and enhanced LLM workflows. It offers a collection of working systems for various domains and complexities, as well as enhanced LLM inference and optimization APIs.\\n\\n5. [AutoGen Studio: Interactively Explore Multi-Agent Workflows](https://microsoft.github.io/autogen/blog/2023/12/01/AutoGenStudio/)\\nAutoGen has emerged as a leading framework for orchestrating the power of agents. In the spirit of expanding this frontier and democratizing this capability, we are thrilled to introduce a new user-friendly interface: AutoGen Studio.\\n\\n6. [[2308.08155] AutoGen: Enabling Next-Gen LLM Applications via Multi ...](https://arxiv.org/abs/2308.08155)\\nAutoGen is an open-source framework that allows developers to create and customize agents that can converse with each other to perform tasks using various types of language models (LLMs). The framework supports natural language and code-based conversation patterns, and is effective for diverse applications such as mathematics, coding, question answering, and more.\\n\\n7. [Mastering AutoGen: A Comprehensive Guide to Next-Generation ... - Medium](https://medium.com/@krtarunsingh/mastering-autogen-a-comprehensive-guide-to-next-generation-language-model-applications-b375d9b4dc6d)\\nAutoGen is a framework by Microsoft that allows you to create applications that leverage large language models (LLMs) with multi-agent conversations, diverse patterns, and enhanced inference. Learn how to set up AutoGen, use its architecture, and apply its features in this comprehensive guide by Tarun Singh.\\n\\n8. [arXiv:2308.08155v2 [cs.AI] 3 Oct 2023](https://arxiv.org/pdf/2308.08155.pdf)\\nAutoGen is an open-source framework that allows developers to create and customize agents that can converse with each other to solve tasks using multiple languages, tools, and human inputs. The framework supports flexible conversation patterns and natural or code-based programming for diverse applications of complexities and LLM capacities.\", 'role': 'user'}, {'content': 'Summarize these results', 'role': 'assistant'}, {'content': 'AutoGen is a framework developed by Microsoft Research to simplify the orchestration, optimization, and automation of large language model (LLM) workflows. The framework offers customizable and conversable agents that utilize advanced LLM capabilities, such as GPT-4, while also integrating with humans and tools to address limitations and enhance performance. As developers create more complex LLM-based applications, the workflows become intricate, requiring significant effort and expertise to design and implement. Automating these workflows using AutoGen can streamline the process and improve efficiency, enabling the creation of next-generation applications that leverage the full potential of LLMs. The framework supports conversations between multiple agents through automated chat, providing a solution to the challenge of orchestrating optimal workflows in a vast and complex design space.', 'role': 'user'}, {'content': \"Click the 'Getting Started' result\", 'role': 'assistant'}, {'content': 'Address: https://microsoft.github.io/autogen/docs/Getting-Started/\\nTitle: Getting Started | AutoGen\\nViewport position: Showing page 1 of 1.\\n=======================\\n\\n\\n\\nGetting Started | AutoGen\\n\\n\\n\\n\\n\\n\\n\\n[Skip to main content](#__docusaurus_skipToContent_fallback)[![AutoGen](/autogen/img/ag.svg)**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)[Resources](#)* [Ecosystem](/autogen/docs/Ecosystem)\\n* [Gallery](/autogen/docs/Gallery)\\n[Other Languages](#)* [Dotnet](https://microsoft.github.io/autogen-for-net/)\\n[GitHub](https://github.com/microsoft/autogen)`⌘``K`* [Getting Started](/autogen/docs/Getting-Started)\\n* [Installation](/autogen/docs/installation/)\\n* [LLM Configuration](/autogen/docs/llm_configuration)\\n* [Use Cases](#)\\n* [Contributing](/autogen/docs/Contribute)\\n* [Research](/autogen/docs/Research)\\n* [Migration Guide](/autogen/docs/Migration-Guide)\\n* \\n* Getting Started\\nOn this pageGetting Started\\n===============\\n\\n\\nAutoGen is a framework that enables development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools.\\n\\n\\n![AutoGen Overview](/autogen/assets/images/autogen_agentchat-250ca64b77b87e70d34766a080bf6ba8.png)\\n\\n\\n### Main Features[\\u200b](#main-features \"Direct link to Main Features\")\\n\\n\\n* AutoGen enables building next-gen LLM applications based on [multi-agent conversations](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat) with minimal effort. It simplifies the orchestration, automation, and optimization of a complex LLM workflow. It maximizes the performance of LLM models and overcomes their weaknesses.\\n* It supports [diverse conversation patterns](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat#supporting-diverse-conversation-patterns) for complex workflows. With customizable and conversable agents, developers can use AutoGen to build a wide range of conversation patterns concerning conversation autonomy,\\nthe number of agents, and agent conversation topology.\\n* It provides a collection of working systems with different complexities. These systems span a [wide range of applications](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat#diverse-applications-implemented-with-autogen) from various domains and complexities. This demonstrates how AutoGen can easily support diverse conversation patterns.\\n* AutoGen provides [enhanced LLM inference](https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference#api-unification). It offers utilities like API unification and caching, and advanced usage patterns, such as error handling, multi-config inference, context programming, etc.\\n\\n\\nAutoGen is powered by collaborative [research studies](/autogen/docs/Research) from Microsoft, Penn State University, and University of Washington.\\n\\n\\n### Quickstart[\\u200b](#quickstart \"Direct link to Quickstart\")\\n\\n\\nInstall from pip: `pip install pyautogen`. Find more options in [Installation](/autogen/docs/installation/).\\nFor [code execution](/autogen/docs/FAQ#code-execution), we strongly recommend installing the python docker package, and using docker.\\n\\n\\n#### Multi-Agent Conversation Framework[\\u200b](#multi-agent-conversation-framework \"Direct link to Multi-Agent Conversation Framework\")\\n\\n\\nAutogen enables the next-gen LLM applications with a generic multi-agent conversation framework. It offers customizable and conversable agents which integrate LLMs, tools, and humans.\\nBy automating chat among multiple capable agents, one can easily make them collectively perform tasks autonomously or with human feedback, including tasks that require using tools via code. For [example](https://github.com/microsoft/autogen/blob/main/test/twoagent.py),\\n\\n\\n\\n```\\nfrom autogen import AssistantAgent, UserProxyAgent, config\\\\_list\\\\_from\\\\_json  \\n  \\n# Load LLM inference endpoints from an env variable or a file  \\n# See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints  \\n# and OAI\\\\_CONFIG\\\\_LIST\\\\_sample.json  \\nconfig\\\\_list = config\\\\_list\\\\_from\\\\_json(env\\\\_or\\\\_file=\"OAI\\\\_CONFIG\\\\_LIST\")  \\nassistant = AssistantAgent(\"assistant\", llm\\\\_config={\"config\\\\_list\": config\\\\_list})  \\nuser\\\\_proxy = UserProxyAgent(\"user\\\\_proxy\", code\\\\_execution\\\\_config={\"work\\\\_dir\": \"coding\", \"use\\\\_docker\": False}) # IMPORTANT: set to True to run code in docker, recommended  \\nuser\\\\_proxy.initiate\\\\_chat(assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\")  \\n# This initiates an automated chat between the two agents to solve the task  \\n\\n```\\n\\nThe figure below shows an example conversation flow with AutoGen.\\n![Agent Chat Example](/autogen/assets/images/chat_example-da70a7420ebc817ef9826fa4b1e80951.png)\\n\\n\\n* [Code examples](/autogen/docs/Examples).\\n* [Documentation](/autogen/docs/Use-Cases/agent_chat).\\n\\n\\n#### Enhanced LLM Inferences[\\u200b](#enhanced-llm-inferences \"Direct link to Enhanced LLM Inferences\")\\n\\n\\nAutogen also helps maximize the utility out of the expensive LLMs such as ChatGPT and GPT-4. It offers enhanced LLM inference with powerful functionalities like tuning, caching, error handling, templating. For example, you can optimize generations by LLM with your own tuning data, success metrics and budgets.\\n\\n\\n\\n```\\n# perform tuning for openai<1  \\nconfig, analysis = autogen.Completion.tune(  \\n data=tune\\\\_data,  \\n metric=\"success\",  \\n mode=\"max\",  \\n eval\\\\_func=eval\\\\_func,  \\n inference\\\\_budget=0.05,  \\n optimization\\\\_budget=3,  \\n num\\\\_samples=-1,  \\n)  \\n# perform inference for a test instance  \\nresponse = autogen.Completion.create(context=test\\\\_instance, \\\\*\\\\*config)  \\n\\n```\\n\\n* [Code examples](/autogen/docs/Examples).\\n* [Documentation](/autogen/docs/Use-Cases/enhanced_inference).\\n\\n\\n### Where to Go Next ?[\\u200b](#where-to-go-next- \"Direct link to Where to Go Next ?\")\\n\\n\\n* Understand the use cases for [multi-agent conversation](/autogen/docs/Use-Cases/agent_chat) and [enhanced LLM inference](/autogen/docs/Use-Cases/enhanced_inference).\\n* Find [code examples](/autogen/docs/Examples).\\n* Read [SDK](/autogen/docs/reference/agentchat/conversable_agent/).\\n* Learn about [research](/autogen/docs/Research) around AutoGen.\\n* [Roadmap](https://github.com/orgs/microsoft/projects/989/views/3)\\n* Chat on [Discord](https://discord.gg/pAbnFJrkgZ).\\n* Follow on [Twitter](https://twitter.com/pyautogen).\\n\\n\\nIf you like our project, please give it a [star](https://github.com/microsoft/autogen/stargazers) on GitHub. If you are interested in contributing, please read [Contributor\\'s Guide](/autogen/docs/Contribute).\\n\\n\\n[Edit this page](https://github.com/microsoft/autogen/edit/main/website/docs/Getting-Started.md)[NextInstallation](/autogen/docs/installation/)* [Main Features](#main-features)\\n* [Quickstart](#quickstart)\\n* [Where to Go Next ?](#where-to-go-next-)\\nCommunity* [Discord](https://discord.gg/pAbnFJrkgZ)\\n* [Twitter](https://twitter.com/pyautogen)\\nCopyright © 2024 AutoGen Authors | [Privacy and Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)\\n', 'role': 'user'}], summary='Address: https://microsoft.github.io/autogen/docs/Getting-Started/\\nTitle: Getting Started | AutoGen\\nViewport position: Showing page 1 of 1.\\n=======================\\n\\n\\n\\nGetting Started | AutoGen\\n\\n\\n\\n\\n\\n\\n\\n[Skip to main content](#__docusaurus_skipToContent_fallback)[![AutoGen](/autogen/img/ag.svg)**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)[Resources](#)* [Ecosystem](/autogen/docs/Ecosystem)\\n* [Gallery](/autogen/docs/Gallery)\\n[Other Languages](#)* [Dotnet](https://microsoft.github.io/autogen-for-net/)\\n[GitHub](https://github.com/microsoft/autogen)`⌘``K`* [Getting Started](/autogen/docs/Getting-Started)\\n* [Installation](/autogen/docs/installation/)\\n* [LLM Configuration](/autogen/docs/llm_configuration)\\n* [Use Cases](#)\\n* [Contributing](/autogen/docs/Contribute)\\n* [Research](/autogen/docs/Research)\\n* [Migration Guide](/autogen/docs/Migration-Guide)\\n* \\n* Getting Started\\nOn this pageGetting Started\\n===============\\n\\n\\nAutoGen is a framework that enables development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools.\\n\\n\\n![AutoGen Overview](/autogen/assets/images/autogen_agentchat-250ca64b77b87e70d34766a080bf6ba8.png)\\n\\n\\n### Main Features[\\u200b](#main-features \"Direct link to Main Features\")\\n\\n\\n* AutoGen enables building next-gen LLM applications based on [multi-agent conversations](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat) with minimal effort. It simplifies the orchestration, automation, and optimization of a complex LLM workflow. It maximizes the performance of LLM models and overcomes their weaknesses.\\n* It supports [diverse conversation patterns](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat#supporting-diverse-conversation-patterns) for complex workflows. With customizable and conversable agents, developers can use AutoGen to build a wide range of conversation patterns concerning conversation autonomy,\\nthe number of agents, and agent conversation topology.\\n* It provides a collection of working systems with different complexities. These systems span a [wide range of applications](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat#diverse-applications-implemented-with-autogen) from various domains and complexities. This demonstrates how AutoGen can easily support diverse conversation patterns.\\n* AutoGen provides [enhanced LLM inference](https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference#api-unification). It offers utilities like API unification and caching, and advanced usage patterns, such as error handling, multi-config inference, context programming, etc.\\n\\n\\nAutoGen is powered by collaborative [research studies](/autogen/docs/Research) from Microsoft, Penn State University, and University of Washington.\\n\\n\\n### Quickstart[\\u200b](#quickstart \"Direct link to Quickstart\")\\n\\n\\nInstall from pip: `pip install pyautogen`. Find more options in [Installation](/autogen/docs/installation/).\\nFor [code execution](/autogen/docs/FAQ#code-execution), we strongly recommend installing the python docker package, and using docker.\\n\\n\\n#### Multi-Agent Conversation Framework[\\u200b](#multi-agent-conversation-framework \"Direct link to Multi-Agent Conversation Framework\")\\n\\n\\nAutogen enables the next-gen LLM applications with a generic multi-agent conversation framework. It offers customizable and conversable agents which integrate LLMs, tools, and humans.\\nBy automating chat among multiple capable agents, one can easily make them collectively perform tasks autonomously or with human feedback, including tasks that require using tools via code. For [example](https://github.com/microsoft/autogen/blob/main/test/twoagent.py),\\n\\n\\n\\n```\\nfrom autogen import AssistantAgent, UserProxyAgent, config\\\\_list\\\\_from\\\\_json  \\n  \\n# Load LLM inference endpoints from an env variable or a file  \\n# See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints  \\n# and OAI\\\\_CONFIG\\\\_LIST\\\\_sample.json  \\nconfig\\\\_list = config\\\\_list\\\\_from\\\\_json(env\\\\_or\\\\_file=\"OAI\\\\_CONFIG\\\\_LIST\")  \\nassistant = AssistantAgent(\"assistant\", llm\\\\_config={\"config\\\\_list\": config\\\\_list})  \\nuser\\\\_proxy = UserProxyAgent(\"user\\\\_proxy\", code\\\\_execution\\\\_config={\"work\\\\_dir\": \"coding\", \"use\\\\_docker\": False}) # IMPORTANT: set to True to run code in docker, recommended  \\nuser\\\\_proxy.initiate\\\\_chat(assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\")  \\n# This initiates an automated chat between the two agents to solve the task  \\n\\n```\\n\\nThe figure below shows an example conversation flow with AutoGen.\\n![Agent Chat Example](/autogen/assets/images/chat_example-da70a7420ebc817ef9826fa4b1e80951.png)\\n\\n\\n* [Code examples](/autogen/docs/Examples).\\n* [Documentation](/autogen/docs/Use-Cases/agent_chat).\\n\\n\\n#### Enhanced LLM Inferences[\\u200b](#enhanced-llm-inferences \"Direct link to Enhanced LLM Inferences\")\\n\\n\\nAutogen also helps maximize the utility out of the expensive LLMs such as ChatGPT and GPT-4. It offers enhanced LLM inference with powerful functionalities like tuning, caching, error handling, templating. For example, you can optimize generations by LLM with your own tuning data, success metrics and budgets.\\n\\n\\n\\n```\\n# perform tuning for openai<1  \\nconfig, analysis = autogen.Completion.tune(  \\n data=tune\\\\_data,  \\n metric=\"success\",  \\n mode=\"max\",  \\n eval\\\\_func=eval\\\\_func,  \\n inference\\\\_budget=0.05,  \\n optimization\\\\_budget=3,  \\n num\\\\_samples=-1,  \\n)  \\n# perform inference for a test instance  \\nresponse = autogen.Completion.create(context=test\\\\_instance, \\\\*\\\\*config)  \\n\\n```\\n\\n* [Code examples](/autogen/docs/Examples).\\n* [Documentation](/autogen/docs/Use-Cases/enhanced_inference).\\n\\n\\n### Where to Go Next ?[\\u200b](#where-to-go-next- \"Direct link to Where to Go Next ?\")\\n\\n\\n* Understand the use cases for [multi-agent conversation](/autogen/docs/Use-Cases/agent_chat) and [enhanced LLM inference](/autogen/docs/Use-Cases/enhanced_inference).\\n* Find [code examples](/autogen/docs/Examples).\\n* Read [SDK](/autogen/docs/reference/agentchat/conversable_agent/).\\n* Learn about [research](/autogen/docs/Research) around AutoGen.\\n* [Roadmap](https://github.com/orgs/microsoft/projects/989/views/3)\\n* Chat on [Discord](https://discord.gg/pAbnFJrkgZ).\\n* Follow on [Twitter](https://twitter.com/pyautogen).\\n\\n\\nIf you like our project, please give it a [star](https://github.com/microsoft/autogen/stargazers) on GitHub. If you are interested in contributing, please read [Contributor\\'s Guide](/autogen/docs/Contribute).\\n\\n\\n[Edit this page](https://github.com/microsoft/autogen/edit/main/website/docs/Getting-Started.md)[NextInstallation](/autogen/docs/installation/)* [Main Features](#main-features)\\n* [Quickstart](#quickstart)\\n* [Where to Go Next ?](#where-to-go-next-)\\nCommunity* [Discord](https://discord.gg/pAbnFJrkgZ)\\n* [Twitter](https://twitter.com/pyautogen)\\nCopyright © 2024 AutoGen Authors | [Privacy and Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)\\n', cost=({'total_cost': 0}, {'total_cost': 0}), human_input=[])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task3 = \"Click the 'Getting Started' result\"\n",
    "user_proxy.initiate_chat(web_surfer, message=task3, clear_history=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Let's look at the actual page rendered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_binary_image(web_surfer.browser.driver.get_screenshot_as_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's scroll down and look again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task5 = \"\"\"Scroll down.\"\"\"\n",
    "user_proxy.initiate_chat(web_surfer, message=task5, clear_history=False)\n",
    "\n",
    "# We give it few seconds before viewing the browser\n",
    "sleep(3)\n",
    "display_binary_image(web_surfer.browser.driver.get_screenshot_as_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's test our navigation using the rendered page\n",
    "Note: this does require vision capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to web_surfer):\n",
      "\n",
      "Click the 'research studies' link\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION visit_page...\u001b[0m\n",
      "\u001b[33mweb_surfer\u001b[0m (to user_proxy):\n",
      "\n",
      "Address: https://microsoft.github.io/autogen/docs/Research\n",
      "Title: Research | AutoGen\n",
      "Viewport position: Showing page 1 of 1.\n",
      "=======================\n",
      "\n",
      "\n",
      "\n",
      "Research | AutoGen\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Skip to main content](#__docusaurus_skipToContent_fallback)[![AutoGen](/autogen/img/ag.svg)**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)[Resources](#)* [Ecosystem](/autogen/docs/Ecosystem)\n",
      "* [Gallery](/autogen/docs/Gallery)\n",
      "[Other Languages](#)* [Dotnet](https://microsoft.github.io/autogen-for-net/)\n",
      "[GitHub](https://github.com/microsoft/autogen)`⌘``K`* [Getting Started](/autogen/docs/Getting-Started)\n",
      "* [Installation](/autogen/docs/installation/)\n",
      "* [LLM Configuration](/autogen/docs/llm_configuration)\n",
      "* [Use Cases](#)\n",
      "* [Contributing](/autogen/docs/Contribute)\n",
      "* [Research](/autogen/docs/Research)\n",
      "* [Migration Guide](/autogen/docs/Migration-Guide)\n",
      "* \n",
      "* Research\n",
      "Research\n",
      "========\n",
      "\n",
      "\n",
      "For technical details, please check our technical report and research publications.\n",
      "\n",
      "\n",
      "* [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework](https://arxiv.org/abs/2308.08155). Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang and Chi Wang. ArXiv 2023.\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "@inproceedings{wu2023autogen,  \n",
      " title={AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework},  \n",
      " author={Qingyun Wu and Gagan Bansal and Jieyu Zhang and Yiran Wu and Shaokun Zhang and Erkang Zhu and Beibin Li and Li Jiang and Xiaoyun Zhang and Chi Wang},  \n",
      " year={2023},  \n",
      " eprint={2308.08155},  \n",
      " archivePrefix={arXiv},  \n",
      " primaryClass={cs.AI}  \n",
      "}  \n",
      "\n",
      "```\n",
      "\n",
      "* [Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference](https://arxiv.org/abs/2303.04673). Chi Wang, Susan Xueqing Liu, Ahmed H. Awadallah. AutoML'23.\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "@inproceedings{wang2023EcoOptiGen,  \n",
      " title={Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference},  \n",
      " author={Chi Wang and Susan Xueqing Liu and Ahmed H. Awadallah},  \n",
      " year={2023},  \n",
      " booktitle={AutoML'23},  \n",
      "}  \n",
      "\n",
      "```\n",
      "\n",
      "* [An Empirical Study on Challenging Math Problem Solving with GPT-4](https://arxiv.org/abs/2306.01337). Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, Chi Wang. ArXiv preprint arXiv:2306.01337 (2023).\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "@inproceedings{wu2023empirical,  \n",
      " title={An Empirical Study on Challenging Math Problem Solving with GPT-4},  \n",
      " author={Yiran Wu and Feiran Jia and Shaokun Zhang and Hangyu Li and Erkang Zhu and Yue Wang and Yin Tat Lee and Richard Peng and Qingyun Wu and Chi Wang},  \n",
      " year={2023},  \n",
      " booktitle={ArXiv preprint arXiv:2306.01337},  \n",
      "}  \n",
      "\n",
      "```\n",
      "\n",
      "* [EcoAssistant: Using LLM Assistant More Affordably and Accurately](https://arxiv.org/abs/2310.03046). Jieyu Zhang, Ranjay Krishna, Ahmed H. Awadallah, Chi Wang. ArXiv preprint arXiv:2310.03046 (2023).\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "@inproceedings{zhang2023ecoassistant,  \n",
      " title={EcoAssistant: Using LLM Assistant More Affordably and Accurately},  \n",
      " author={Zhang, Jieyu and Krishna, Ranjay and Awadallah, Ahmed H and Wang, Chi},  \n",
      " year={2023},  \n",
      " booktitle={ArXiv preprint arXiv:2310.03046},  \n",
      "}  \n",
      "\n",
      "```\n",
      "[Edit this page](https://github.com/microsoft/autogen/edit/main/website/docs/Research.md)[PreviousContributing](/autogen/docs/Contribute)[NextMigration Guide](/autogen/docs/Migration-Guide)Community* [Discord](https://discord.gg/pAbnFJrkgZ)\n",
      "* [Twitter](https://twitter.com/pyautogen)\n",
      "Copyright © 2024 AutoGen Authors | [Privacy and Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_history=[{'content': '\\nSearch the web for information about Microsoft AutoGen\\n', 'role': 'assistant'}, {'content': \"Address: bing: Microsoft AutoGen\\nTitle: Microsoft AutoGen - Search\\nViewport position: Showing page 1 of 1.\\n=======================\\nA Bing search for 'Microsoft AutoGen' found 8 results:\\n\\n## Web Results\\n1. [AutoGen: Enabling next-generation large language model applications](https://www.microsoft.com/en-us/research/blog/autogen-enabling-next-generation-large-language-model-applications/)\\nAutoGen is a Python package that simplifies the orchestration, optimization, and automation of large language model applications. It enables customizable and conversable agents that integrate with humans, tools, and other agents to solve tasks using GPT-4 and other advanced LLMs. Learn how to use AutoGen for code-based question answering, supply-chain optimization, conversational chess, and more.\\n\\n2. [GitHub - microsoft/autogen: Enable Next-Gen Large Language Model ...](https://github.com/microsoft/autogen)\\nAutoGen is a framework that enables the development of large language model applications using multiple agents that can converse with each other to solve tasks. It supports diverse conversation patterns, enhanced LLM inference, and customizable and conversable agents.\\n\\n3. [Getting Started | AutoGen - microsoft.github.io](https://microsoft.github.io/autogen/docs/Getting-Started/)\\nAutoGen is a framework that enables development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools. Main Features\\n\\n4. [AutoGen | AutoGen - microsoft.github.io](https://microsoft.github.io/autogen/)\\nAutoGen is a tool that enables next-gen large language model applications by providing a high-level abstraction for building diverse and enhanced LLM workflows. It offers a collection of working systems for various domains and complexities, as well as enhanced LLM inference and optimization APIs.\\n\\n5. [AutoGen Studio: Interactively Explore Multi-Agent Workflows](https://microsoft.github.io/autogen/blog/2023/12/01/AutoGenStudio/)\\nAutoGen has emerged as a leading framework for orchestrating the power of agents. In the spirit of expanding this frontier and democratizing this capability, we are thrilled to introduce a new user-friendly interface: AutoGen Studio.\\n\\n6. [[2308.08155] AutoGen: Enabling Next-Gen LLM Applications via Multi ...](https://arxiv.org/abs/2308.08155)\\nAutoGen is an open-source framework that allows developers to create and customize agents that can converse with each other to perform tasks using various types of language models (LLMs). The framework supports natural language and code-based conversation patterns, and is effective for diverse applications such as mathematics, coding, question answering, and more.\\n\\n7. [Mastering AutoGen: A Comprehensive Guide to Next-Generation ... - Medium](https://medium.com/@krtarunsingh/mastering-autogen-a-comprehensive-guide-to-next-generation-language-model-applications-b375d9b4dc6d)\\nAutoGen is a framework by Microsoft that allows you to create applications that leverage large language models (LLMs) with multi-agent conversations, diverse patterns, and enhanced inference. Learn how to set up AutoGen, use its architecture, and apply its features in this comprehensive guide by Tarun Singh.\\n\\n8. [arXiv:2308.08155v2 [cs.AI] 3 Oct 2023](https://arxiv.org/pdf/2308.08155.pdf)\\nAutoGen is an open-source framework that allows developers to create and customize agents that can converse with each other to solve tasks using multiple languages, tools, and human inputs. The framework supports flexible conversation patterns and natural or code-based programming for diverse applications of complexities and LLM capacities.\", 'role': 'user'}, {'content': 'Summarize these results', 'role': 'assistant'}, {'content': 'AutoGen is a framework developed by Microsoft Research to simplify the orchestration, optimization, and automation of large language model (LLM) workflows. The framework offers customizable and conversable agents that utilize advanced LLM capabilities, such as GPT-4, while also integrating with humans and tools to address limitations and enhance performance. As developers create more complex LLM-based applications, the workflows become intricate, requiring significant effort and expertise to design and implement. Automating these workflows using AutoGen can streamline the process and improve efficiency, enabling the creation of next-generation applications that leverage the full potential of LLMs. The framework supports conversations between multiple agents through automated chat, providing a solution to the challenge of orchestrating optimal workflows in a vast and complex design space.', 'role': 'user'}, {'content': \"Click the 'Getting Started' result\", 'role': 'assistant'}, {'content': 'Address: https://microsoft.github.io/autogen/docs/Getting-Started/\\nTitle: Getting Started | AutoGen\\nViewport position: Showing page 1 of 1.\\n=======================\\n\\n\\n\\nGetting Started | AutoGen\\n\\n\\n\\n\\n\\n\\n\\n[Skip to main content](#__docusaurus_skipToContent_fallback)[![AutoGen](/autogen/img/ag.svg)**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)[Resources](#)* [Ecosystem](/autogen/docs/Ecosystem)\\n* [Gallery](/autogen/docs/Gallery)\\n[Other Languages](#)* [Dotnet](https://microsoft.github.io/autogen-for-net/)\\n[GitHub](https://github.com/microsoft/autogen)`⌘``K`* [Getting Started](/autogen/docs/Getting-Started)\\n* [Installation](/autogen/docs/installation/)\\n* [LLM Configuration](/autogen/docs/llm_configuration)\\n* [Use Cases](#)\\n* [Contributing](/autogen/docs/Contribute)\\n* [Research](/autogen/docs/Research)\\n* [Migration Guide](/autogen/docs/Migration-Guide)\\n* \\n* Getting Started\\nOn this pageGetting Started\\n===============\\n\\n\\nAutoGen is a framework that enables development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools.\\n\\n\\n![AutoGen Overview](/autogen/assets/images/autogen_agentchat-250ca64b77b87e70d34766a080bf6ba8.png)\\n\\n\\n### Main Features[\\u200b](#main-features \"Direct link to Main Features\")\\n\\n\\n* AutoGen enables building next-gen LLM applications based on [multi-agent conversations](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat) with minimal effort. It simplifies the orchestration, automation, and optimization of a complex LLM workflow. It maximizes the performance of LLM models and overcomes their weaknesses.\\n* It supports [diverse conversation patterns](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat#supporting-diverse-conversation-patterns) for complex workflows. With customizable and conversable agents, developers can use AutoGen to build a wide range of conversation patterns concerning conversation autonomy,\\nthe number of agents, and agent conversation topology.\\n* It provides a collection of working systems with different complexities. These systems span a [wide range of applications](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat#diverse-applications-implemented-with-autogen) from various domains and complexities. This demonstrates how AutoGen can easily support diverse conversation patterns.\\n* AutoGen provides [enhanced LLM inference](https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference#api-unification). It offers utilities like API unification and caching, and advanced usage patterns, such as error handling, multi-config inference, context programming, etc.\\n\\n\\nAutoGen is powered by collaborative [research studies](/autogen/docs/Research) from Microsoft, Penn State University, and University of Washington.\\n\\n\\n### Quickstart[\\u200b](#quickstart \"Direct link to Quickstart\")\\n\\n\\nInstall from pip: `pip install pyautogen`. Find more options in [Installation](/autogen/docs/installation/).\\nFor [code execution](/autogen/docs/FAQ#code-execution), we strongly recommend installing the python docker package, and using docker.\\n\\n\\n#### Multi-Agent Conversation Framework[\\u200b](#multi-agent-conversation-framework \"Direct link to Multi-Agent Conversation Framework\")\\n\\n\\nAutogen enables the next-gen LLM applications with a generic multi-agent conversation framework. It offers customizable and conversable agents which integrate LLMs, tools, and humans.\\nBy automating chat among multiple capable agents, one can easily make them collectively perform tasks autonomously or with human feedback, including tasks that require using tools via code. For [example](https://github.com/microsoft/autogen/blob/main/test/twoagent.py),\\n\\n\\n\\n```\\nfrom autogen import AssistantAgent, UserProxyAgent, config\\\\_list\\\\_from\\\\_json  \\n  \\n# Load LLM inference endpoints from an env variable or a file  \\n# See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints  \\n# and OAI\\\\_CONFIG\\\\_LIST\\\\_sample.json  \\nconfig\\\\_list = config\\\\_list\\\\_from\\\\_json(env\\\\_or\\\\_file=\"OAI\\\\_CONFIG\\\\_LIST\")  \\nassistant = AssistantAgent(\"assistant\", llm\\\\_config={\"config\\\\_list\": config\\\\_list})  \\nuser\\\\_proxy = UserProxyAgent(\"user\\\\_proxy\", code\\\\_execution\\\\_config={\"work\\\\_dir\": \"coding\", \"use\\\\_docker\": False}) # IMPORTANT: set to True to run code in docker, recommended  \\nuser\\\\_proxy.initiate\\\\_chat(assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\")  \\n# This initiates an automated chat between the two agents to solve the task  \\n\\n```\\n\\nThe figure below shows an example conversation flow with AutoGen.\\n![Agent Chat Example](/autogen/assets/images/chat_example-da70a7420ebc817ef9826fa4b1e80951.png)\\n\\n\\n* [Code examples](/autogen/docs/Examples).\\n* [Documentation](/autogen/docs/Use-Cases/agent_chat).\\n\\n\\n#### Enhanced LLM Inferences[\\u200b](#enhanced-llm-inferences \"Direct link to Enhanced LLM Inferences\")\\n\\n\\nAutogen also helps maximize the utility out of the expensive LLMs such as ChatGPT and GPT-4. It offers enhanced LLM inference with powerful functionalities like tuning, caching, error handling, templating. For example, you can optimize generations by LLM with your own tuning data, success metrics and budgets.\\n\\n\\n\\n```\\n# perform tuning for openai<1  \\nconfig, analysis = autogen.Completion.tune(  \\n data=tune\\\\_data,  \\n metric=\"success\",  \\n mode=\"max\",  \\n eval\\\\_func=eval\\\\_func,  \\n inference\\\\_budget=0.05,  \\n optimization\\\\_budget=3,  \\n num\\\\_samples=-1,  \\n)  \\n# perform inference for a test instance  \\nresponse = autogen.Completion.create(context=test\\\\_instance, \\\\*\\\\*config)  \\n\\n```\\n\\n* [Code examples](/autogen/docs/Examples).\\n* [Documentation](/autogen/docs/Use-Cases/enhanced_inference).\\n\\n\\n### Where to Go Next ?[\\u200b](#where-to-go-next- \"Direct link to Where to Go Next ?\")\\n\\n\\n* Understand the use cases for [multi-agent conversation](/autogen/docs/Use-Cases/agent_chat) and [enhanced LLM inference](/autogen/docs/Use-Cases/enhanced_inference).\\n* Find [code examples](/autogen/docs/Examples).\\n* Read [SDK](/autogen/docs/reference/agentchat/conversable_agent/).\\n* Learn about [research](/autogen/docs/Research) around AutoGen.\\n* [Roadmap](https://github.com/orgs/microsoft/projects/989/views/3)\\n* Chat on [Discord](https://discord.gg/pAbnFJrkgZ).\\n* Follow on [Twitter](https://twitter.com/pyautogen).\\n\\n\\nIf you like our project, please give it a [star](https://github.com/microsoft/autogen/stargazers) on GitHub. If you are interested in contributing, please read [Contributor\\'s Guide](/autogen/docs/Contribute).\\n\\n\\n[Edit this page](https://github.com/microsoft/autogen/edit/main/website/docs/Getting-Started.md)[NextInstallation](/autogen/docs/installation/)* [Main Features](#main-features)\\n* [Quickstart](#quickstart)\\n* [Where to Go Next ?](#where-to-go-next-)\\nCommunity* [Discord](https://discord.gg/pAbnFJrkgZ)\\n* [Twitter](https://twitter.com/pyautogen)\\nCopyright © 2024 AutoGen Authors | [Privacy and Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)\\n', 'role': 'user'}, {'content': 'Scroll down.', 'role': 'assistant'}, {'content': 'Address: https://microsoft.github.io/autogen/docs/Getting-Started/\\nTitle: Getting Started | AutoGen\\nViewport position: Showing page 1 of 1.\\n=======================\\n\\n\\n\\nGetting Started | AutoGen\\n\\n\\n\\n\\n\\n\\n\\n[Skip to main content](#__docusaurus_skipToContent_fallback)[![AutoGen](/autogen/img/ag.svg)**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)[Resources](#)* [Ecosystem](/autogen/docs/Ecosystem)\\n* [Gallery](/autogen/docs/Gallery)\\n[Other Languages](#)* [Dotnet](https://microsoft.github.io/autogen-for-net/)\\n[GitHub](https://github.com/microsoft/autogen)`⌘``K`* [Getting Started](/autogen/docs/Getting-Started)\\n* [Installation](/autogen/docs/installation/)\\n* [LLM Configuration](/autogen/docs/llm_configuration)\\n* [Use Cases](#)\\n* [Contributing](/autogen/docs/Contribute)\\n* [Research](/autogen/docs/Research)\\n* [Migration Guide](/autogen/docs/Migration-Guide)\\n* \\n* Getting Started\\nOn this pageGetting Started\\n===============\\n\\n\\nAutoGen is a framework that enables development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools.\\n\\n\\n![AutoGen Overview](/autogen/assets/images/autogen_agentchat-250ca64b77b87e70d34766a080bf6ba8.png)\\n\\n\\n### Main Features[\\u200b](#main-features \"Direct link to Main Features\")\\n\\n\\n* AutoGen enables building next-gen LLM applications based on [multi-agent conversations](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat) with minimal effort. It simplifies the orchestration, automation, and optimization of a complex LLM workflow. It maximizes the performance of LLM models and overcomes their weaknesses.\\n* It supports [diverse conversation patterns](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat#supporting-diverse-conversation-patterns) for complex workflows. With customizable and conversable agents, developers can use AutoGen to build a wide range of conversation patterns concerning conversation autonomy,\\nthe number of agents, and agent conversation topology.\\n* It provides a collection of working systems with different complexities. These systems span a [wide range of applications](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat#diverse-applications-implemented-with-autogen) from various domains and complexities. This demonstrates how AutoGen can easily support diverse conversation patterns.\\n* AutoGen provides [enhanced LLM inference](https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference#api-unification). It offers utilities like API unification and caching, and advanced usage patterns, such as error handling, multi-config inference, context programming, etc.\\n\\n\\nAutoGen is powered by collaborative [research studies](/autogen/docs/Research) from Microsoft, Penn State University, and University of Washington.\\n\\n\\n### Quickstart[\\u200b](#quickstart \"Direct link to Quickstart\")\\n\\n\\nInstall from pip: `pip install pyautogen`. Find more options in [Installation](/autogen/docs/installation/).\\nFor [code execution](/autogen/docs/FAQ#code-execution), we strongly recommend installing the python docker package, and using docker.\\n\\n\\n#### Multi-Agent Conversation Framework[\\u200b](#multi-agent-conversation-framework \"Direct link to Multi-Agent Conversation Framework\")\\n\\n\\nAutogen enables the next-gen LLM applications with a generic multi-agent conversation framework. It offers customizable and conversable agents which integrate LLMs, tools, and humans.\\nBy automating chat among multiple capable agents, one can easily make them collectively perform tasks autonomously or with human feedback, including tasks that require using tools via code. For [example](https://github.com/microsoft/autogen/blob/main/test/twoagent.py),\\n\\n\\n\\n```\\nfrom autogen import AssistantAgent, UserProxyAgent, config\\\\_list\\\\_from\\\\_json  \\n  \\n# Load LLM inference endpoints from an env variable or a file  \\n# See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints  \\n# and OAI\\\\_CONFIG\\\\_LIST\\\\_sample.json  \\nconfig\\\\_list = config\\\\_list\\\\_from\\\\_json(env\\\\_or\\\\_file=\"OAI\\\\_CONFIG\\\\_LIST\")  \\nassistant = AssistantAgent(\"assistant\", llm\\\\_config={\"config\\\\_list\": config\\\\_list})  \\nuser\\\\_proxy = UserProxyAgent(\"user\\\\_proxy\", code\\\\_execution\\\\_config={\"work\\\\_dir\": \"coding\", \"use\\\\_docker\": False}) # IMPORTANT: set to True to run code in docker, recommended  \\nuser\\\\_proxy.initiate\\\\_chat(assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\")  \\n# This initiates an automated chat between the two agents to solve the task  \\n\\n```\\n\\nThe figure below shows an example conversation flow with AutoGen.\\n![Agent Chat Example](/autogen/assets/images/chat_example-da70a7420ebc817ef9826fa4b1e80951.png)\\n\\n\\n* [Code examples](/autogen/docs/Examples).\\n* [Documentation](/autogen/docs/Use-Cases/agent_chat).\\n\\n\\n#### Enhanced LLM Inferences[\\u200b](#enhanced-llm-inferences \"Direct link to Enhanced LLM Inferences\")\\n\\n\\nAutogen also helps maximize the utility out of the expensive LLMs such as ChatGPT and GPT-4. It offers enhanced LLM inference with powerful functionalities like tuning, caching, error handling, templating. For example, you can optimize generations by LLM with your own tuning data, success metrics and budgets.\\n\\n\\n\\n```\\n# perform tuning for openai<1  \\nconfig, analysis = autogen.Completion.tune(  \\n data=tune\\\\_data,  \\n metric=\"success\",  \\n mode=\"max\",  \\n eval\\\\_func=eval\\\\_func,  \\n inference\\\\_budget=0.05,  \\n optimization\\\\_budget=3,  \\n num\\\\_samples=-1,  \\n)  \\n# perform inference for a test instance  \\nresponse = autogen.Completion.create(context=test\\\\_instance, \\\\*\\\\*config)  \\n\\n```\\n\\n* [Code examples](/autogen/docs/Examples).\\n* [Documentation](/autogen/docs/Use-Cases/enhanced_inference).\\n\\n\\n### Where to Go Next ?[\\u200b](#where-to-go-next- \"Direct link to Where to Go Next ?\")\\n\\n\\n* Understand the use cases for [multi-agent conversation](/autogen/docs/Use-Cases/agent_chat) and [enhanced LLM inference](/autogen/docs/Use-Cases/enhanced_inference).\\n* Find [code examples](/autogen/docs/Examples).\\n* Read [SDK](/autogen/docs/reference/agentchat/conversable_agent/).\\n* Learn about [research](/autogen/docs/Research) around AutoGen.\\n* [Roadmap](https://github.com/orgs/microsoft/projects/989/views/3)\\n* Chat on [Discord](https://discord.gg/pAbnFJrkgZ).\\n* Follow on [Twitter](https://twitter.com/pyautogen).\\n\\n\\nIf you like our project, please give it a [star](https://github.com/microsoft/autogen/stargazers) on GitHub. If you are interested in contributing, please read [Contributor\\'s Guide](/autogen/docs/Contribute).\\n\\n\\n[Edit this page](https://github.com/microsoft/autogen/edit/main/website/docs/Getting-Started.md)[NextInstallation](/autogen/docs/installation/)* [Main Features](#main-features)\\n* [Quickstart](#quickstart)\\n* [Where to Go Next ?](#where-to-go-next-)\\nCommunity* [Discord](https://discord.gg/pAbnFJrkgZ)\\n* [Twitter](https://twitter.com/pyautogen)\\nCopyright © 2024 AutoGen Authors | [Privacy and Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)\\n', 'role': 'user'}, {'content': \"Click the 'research studies' link\", 'role': 'assistant'}, {'content': \"Address: https://microsoft.github.io/autogen/docs/Research\\nTitle: Research | AutoGen\\nViewport position: Showing page 1 of 1.\\n=======================\\n\\n\\n\\nResearch | AutoGen\\n\\n\\n\\n\\n\\n\\n\\n[Skip to main content](#__docusaurus_skipToContent_fallback)[![AutoGen](/autogen/img/ag.svg)**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)[Resources](#)* [Ecosystem](/autogen/docs/Ecosystem)\\n* [Gallery](/autogen/docs/Gallery)\\n[Other Languages](#)* [Dotnet](https://microsoft.github.io/autogen-for-net/)\\n[GitHub](https://github.com/microsoft/autogen)`⌘``K`* [Getting Started](/autogen/docs/Getting-Started)\\n* [Installation](/autogen/docs/installation/)\\n* [LLM Configuration](/autogen/docs/llm_configuration)\\n* [Use Cases](#)\\n* [Contributing](/autogen/docs/Contribute)\\n* [Research](/autogen/docs/Research)\\n* [Migration Guide](/autogen/docs/Migration-Guide)\\n* \\n* Research\\nResearch\\n========\\n\\n\\nFor technical details, please check our technical report and research publications.\\n\\n\\n* [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework](https://arxiv.org/abs/2308.08155). Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang and Chi Wang. ArXiv 2023.\\n\\n\\n\\n```\\n@inproceedings{wu2023autogen,  \\n title={AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework},  \\n author={Qingyun Wu and Gagan Bansal and Jieyu Zhang and Yiran Wu and Shaokun Zhang and Erkang Zhu and Beibin Li and Li Jiang and Xiaoyun Zhang and Chi Wang},  \\n year={2023},  \\n eprint={2308.08155},  \\n archivePrefix={arXiv},  \\n primaryClass={cs.AI}  \\n}  \\n\\n```\\n\\n* [Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference](https://arxiv.org/abs/2303.04673). Chi Wang, Susan Xueqing Liu, Ahmed H. Awadallah. AutoML'23.\\n\\n\\n\\n```\\n@inproceedings{wang2023EcoOptiGen,  \\n title={Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference},  \\n author={Chi Wang and Susan Xueqing Liu and Ahmed H. Awadallah},  \\n year={2023},  \\n booktitle={AutoML'23},  \\n}  \\n\\n```\\n\\n* [An Empirical Study on Challenging Math Problem Solving with GPT-4](https://arxiv.org/abs/2306.01337). Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, Chi Wang. ArXiv preprint arXiv:2306.01337 (2023).\\n\\n\\n\\n```\\n@inproceedings{wu2023empirical,  \\n title={An Empirical Study on Challenging Math Problem Solving with GPT-4},  \\n author={Yiran Wu and Feiran Jia and Shaokun Zhang and Hangyu Li and Erkang Zhu and Yue Wang and Yin Tat Lee and Richard Peng and Qingyun Wu and Chi Wang},  \\n year={2023},  \\n booktitle={ArXiv preprint arXiv:2306.01337},  \\n}  \\n\\n```\\n\\n* [EcoAssistant: Using LLM Assistant More Affordably and Accurately](https://arxiv.org/abs/2310.03046). Jieyu Zhang, Ranjay Krishna, Ahmed H. Awadallah, Chi Wang. ArXiv preprint arXiv:2310.03046 (2023).\\n\\n\\n\\n```\\n@inproceedings{zhang2023ecoassistant,  \\n title={EcoAssistant: Using LLM Assistant More Affordably and Accurately},  \\n author={Zhang, Jieyu and Krishna, Ranjay and Awadallah, Ahmed H and Wang, Chi},  \\n year={2023},  \\n booktitle={ArXiv preprint arXiv:2310.03046},  \\n}  \\n\\n```\\n[Edit this page](https://github.com/microsoft/autogen/edit/main/website/docs/Research.md)[PreviousContributing](/autogen/docs/Contribute)[NextMigration Guide](/autogen/docs/Migration-Guide)Community* [Discord](https://discord.gg/pAbnFJrkgZ)\\n* [Twitter](https://twitter.com/pyautogen)\\nCopyright © 2024 AutoGen Authors | [Privacy and Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)\\n\", 'role': 'user'}], summary=\"Address: https://microsoft.github.io/autogen/docs/Research\\nTitle: Research | AutoGen\\nViewport position: Showing page 1 of 1.\\n=======================\\n\\n\\n\\nResearch | AutoGen\\n\\n\\n\\n\\n\\n\\n\\n[Skip to main content](#__docusaurus_skipToContent_fallback)[![AutoGen](/autogen/img/ag.svg)**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-Started)[SDK](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)[Resources](#)* [Ecosystem](/autogen/docs/Ecosystem)\\n* [Gallery](/autogen/docs/Gallery)\\n[Other Languages](#)* [Dotnet](https://microsoft.github.io/autogen-for-net/)\\n[GitHub](https://github.com/microsoft/autogen)`⌘``K`* [Getting Started](/autogen/docs/Getting-Started)\\n* [Installation](/autogen/docs/installation/)\\n* [LLM Configuration](/autogen/docs/llm_configuration)\\n* [Use Cases](#)\\n* [Contributing](/autogen/docs/Contribute)\\n* [Research](/autogen/docs/Research)\\n* [Migration Guide](/autogen/docs/Migration-Guide)\\n* \\n* Research\\nResearch\\n========\\n\\n\\nFor technical details, please check our technical report and research publications.\\n\\n\\n* [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework](https://arxiv.org/abs/2308.08155). Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang and Chi Wang. ArXiv 2023.\\n\\n\\n\\n```\\n@inproceedings{wu2023autogen,  \\n title={AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework},  \\n author={Qingyun Wu and Gagan Bansal and Jieyu Zhang and Yiran Wu and Shaokun Zhang and Erkang Zhu and Beibin Li and Li Jiang and Xiaoyun Zhang and Chi Wang},  \\n year={2023},  \\n eprint={2308.08155},  \\n archivePrefix={arXiv},  \\n primaryClass={cs.AI}  \\n}  \\n\\n```\\n\\n* [Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference](https://arxiv.org/abs/2303.04673). Chi Wang, Susan Xueqing Liu, Ahmed H. Awadallah. AutoML'23.\\n\\n\\n\\n```\\n@inproceedings{wang2023EcoOptiGen,  \\n title={Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference},  \\n author={Chi Wang and Susan Xueqing Liu and Ahmed H. Awadallah},  \\n year={2023},  \\n booktitle={AutoML'23},  \\n}  \\n\\n```\\n\\n* [An Empirical Study on Challenging Math Problem Solving with GPT-4](https://arxiv.org/abs/2306.01337). Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, Chi Wang. ArXiv preprint arXiv:2306.01337 (2023).\\n\\n\\n\\n```\\n@inproceedings{wu2023empirical,  \\n title={An Empirical Study on Challenging Math Problem Solving with GPT-4},  \\n author={Yiran Wu and Feiran Jia and Shaokun Zhang and Hangyu Li and Erkang Zhu and Yue Wang and Yin Tat Lee and Richard Peng and Qingyun Wu and Chi Wang},  \\n year={2023},  \\n booktitle={ArXiv preprint arXiv:2306.01337},  \\n}  \\n\\n```\\n\\n* [EcoAssistant: Using LLM Assistant More Affordably and Accurately](https://arxiv.org/abs/2310.03046). Jieyu Zhang, Ranjay Krishna, Ahmed H. Awadallah, Chi Wang. ArXiv preprint arXiv:2310.03046 (2023).\\n\\n\\n\\n```\\n@inproceedings{zhang2023ecoassistant,  \\n title={EcoAssistant: Using LLM Assistant More Affordably and Accurately},  \\n author={Zhang, Jieyu and Krishna, Ranjay and Awadallah, Ahmed H and Wang, Chi},  \\n year={2023},  \\n booktitle={ArXiv preprint arXiv:2310.03046},  \\n}  \\n\\n```\\n[Edit this page](https://github.com/microsoft/autogen/edit/main/website/docs/Research.md)[PreviousContributing](/autogen/docs/Contribute)[NextMigration Guide](/autogen/docs/Migration-Guide)Community* [Discord](https://discord.gg/pAbnFJrkgZ)\\n* [Twitter](https://twitter.com/pyautogen)\\nCopyright © 2024 AutoGen Authors | [Privacy and Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)\\n\", cost=({'total_cost': 0}, {'total_cost': 0}), human_input=[])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task6 = \"Click the 'research studies' link\"\n",
    "user_proxy.initiate_chat(web_surfer, message=task6, clear_history=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show us the results of that action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_binary_image(web_surfer.browser.driver.get_screenshot_as_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazing!  Agent navigation on the web still works with the full desktop browser which is great news!\n",
    "### And we can always still display the text on screen if our use-case benefited from that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_binary_image(web_surfer.browser.driver.get_screenshot_as_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup process\n",
    "To ensure that we have no lingering processes in the background, we can shutdown the browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gracefully shut down our headless desktop browser\n",
    "web_surfer.close_the_browser()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
